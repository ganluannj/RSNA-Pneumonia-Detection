# Tutorial for the notebook

## 0 Note
Both the notebook and this file contains some explanation/tutorial. Basically, the notebook contains some simple explanation of the code and some direct interpretations of the output. This tutorial includes the explanation of several concepts which may help to understand the notebook.  <br />

## 1 Data 

### 1.1 Data Uploading
Different from the two challenges we did for midterm project, the data size for this challenge is huge (greater than 3GB) and data are medical image. For midterm project I used local machine (my PC) to run the code, however it is not quite possible to use my PC to run code for this challenge. I used GPU provided by Google Colaboratory instead. Then the first thing we need is to upload the data into google Colab. 

I have tried three different ways for this. The first is to downloaded the data from kaggle into my local computer and then upload the data to Colab. The upload speed is very slow. I tried several times, not successfully uploaded all the data for even once. The second is to store data into Google drive and mount Google drive files directly from Colab. This is faster than uploading from local computer, but not fast enough. It is slowed down by downloading data from google drive to Colab. Also I noticed that it is not stable and sometimes the input/output error can pop out due to the number of files under one folder is too many. The last data uploading method is to directly download data from kaggle into Colab, and this is also the method I finally selected. The downloading speed is fast, it is stable, and the running speed is also fast. 

### 1.2 Data Generator
Before discussing data generator I would like to talk about Data augmentation. The best way to make a machine learning model to generalize better is to train it on more data. One way to get more data is to create fake data and add it to the training set. This method is data augmentation. It has been a very effective technique for object detection and segmentation. For image data augmentation, some common used techniques includes: translating the training images a few pixels in each direction, rotaing the image or scaling image. Also alerting the image by augmentation introudce some variation and can be treated as a regularization method. This will further help to reduce the generation error. 

To train our model, we can directly feed our training data into the training model all at once. However, this requires that our entire training set can fit into RAM and no data augmentation involoved. However, for this challenge here, the data set is too large and cannot be fitted into memory all at once. Also to get a better model performance, we want to apply data augmentation. Thus we need data generator. 

For Data generator, we start by initializing the number of epoches we are going to train our model and the batch size. Here is the epochs selected is 25 and batch size is 32. Both of these two hyperparameters are limitted by the fact that Colab can only run upto 12 hours and also the memory of Colab. We read each image data as a numpy array. For training and validataion data, a mask array with the same size of the image was also created to represent the bounding box. The values in mask arary of locations corresponding to bounding area are 1 while the rest are 0.  Both the images and masks are resized from (1024, 1024) to (256, 256) to reduce the running time. Also I did a horizontal flip for half of the images as the augmentation. 

## 2 Model
### 2.1 Convolutional Neutral Network
As we learned in class, convolutional neural network (CNN) is a specialized kind of neural network for processing data that a known grid-like topology, such as iamge data. Convolutional networks are simply neural networks that use convolution in place of general matrix mulitiplication in at least one of there layers. CNN has several advantages, such as sparse interactions, parameter sharing, and invariant to translations. The basic CNN architecture includes convluation layers, pooling layers, and one fully connected layer. I used CNN to build the model. 

### 2.2 Model Architecture
The total architecture is composed by blocks as shown below. The input layer is followed by a convolutional layer. After that there are 4 repeats of the repeating blocks. The repeating block contains one block_1 and two block_2 (explained below). The last is the output layer. The sigmoid activation function is used for the output layer. It produces the probability of each pixel of the resized image being in the bounding box. 
![CNN-2](https://user-images.githubusercontent.com/47232632/57198281-edd00800-6f3e-11e9-9263-702cc170187b.png)

The structure of block 1 is shown below (Please note that for a better visual effect, the layer and size are not drawn based on real values, same for block 2). The input layer is followed by a batch normalization layer. Batch normalization is a technique for improving  the speed, performance, and stability of the neural network. It is achieved through a normalization step that fixes the means and variances of each layer's inputs. It can reduce internal covariate shift. Besides this, with batch normalization layer, the model can use higher learning rate without vanishing or exploding gradients. Furthermore, batch normalization regularizes the network such that it is easier to generalize, and it is thus unnecessary to use dropout to mitigate overfitting. The network also becomes more robust to different initialization schemes and learning rates. Please refer to [wiki](https://en.wikipedia.org/wiki/Batch_normalization) for more details. 

This Batch normalization layer is followed by a Leaky ReLu layer. Same as ReLu, Leaky ReLu is also an nonlinear layer. For Relu, f(x) = max(0, x), thus ReLu tranform all the negative values to 0. One downside with ReLu is the 'dying ReLu', which refer to the problem that one neuron is stuck in the negative part and output is always 0. Because the gradient for the negative part is 0, so once a neuron get negative, it is very hard for it to recover. Such neuron basically palys nothing in the network. Leaky Relu updated the ReLu by output values for negative x as *alpha * x*. The hyperparameter alpha is a small constant . This Leaky ReLu solves the 'dying ReLu' problem. It is also shown that Leaky ReLu can speed up the training (Please refer to [guide](https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7) for more information about Leaky ReLu). However, one interesting thing to notice is that aftering trying several values for the hyperparameter alpha in Leaky ReLu, alpha=0 (Leaky ReLu becomes ReLu in this case) gives the best prediction result. So actually the real activation function used is ReLu. However, as a reminder for myself, I still keep it as Leaky ReLu in the architecture.  The Leaky ReLu layer is followed by a convolution layer and a Max pooling layer. 
![CNN-4](https://user-images.githubusercontent.com/47232632/57198648-21149600-6f43-11e9-9a34-e35a1d4fc92f.png)


The structure of block 2 is shown below. Clearly the architecture is Batch Normalization - Leaky ReLu - Convolution - Batch Normalization - Leaky ReLu - Convolution. As mentioned above, for a better visual effect, the size and depth of each layer is not drawn corrording their real values. There is a pooling layer in block 1 and the pooling size is 2. Thus after one repeat of the repeating block, the size of the feature map is reduced by 1/2. The initial number of kernels is 32. Then it doubles after each repeat. Thus the depth depth doubles after each repeat. 
![CNN-5](https://user-images.githubusercontent.com/47232632/57199150-cd597b00-6f49-11e9-994e-7af34fa35a14.png)

### 2.3 Model training 
The total training images are randomly divided into the training dataset (90%) and validation dataset (10%). 
#### 2.3.1 Loss Function and Metric
The loss function to be optimized is composed by two parts: the Interaction over Union loss (IoU) and the binary crossentropy loss. IoU is commonly used in image segmentation or bounding box problems. The idea is shown below. The red box represents the ground truth and the black box represent the predicted box. IoU equals to the ratio of intersectin of these two boxes over the union of these two boxes. Easy to see that the higher IoU is, the better the predicted result. *1-IoU* was used as the IoU loss. Please note that how IoU calculated specifically in this challenge is slightly different. The reason is that the prediction of the model is not bounding box, but probability of being in the bounding box for each pixel. To calculate IoU, the interaction was calculated as the sum of predicted probability of pixels in the true bounding box. The interaction was calcuated as the number of pixels in true bounding box plus the predicted probability of all pixels minus the interaction. The IoU is still calculated as the interactio over union. 

<img width="289" alt="IoU" src="https://user-images.githubusercontent.com/47232632/57199522-e9f7b200-6f4d-11e9-9696-1f400b7d61c6.png">

The crossentropy loss is commonly used loss for classification. The boxing problem can be regarded as a classficiation problem in the sense that the boxed locations are 1 while the rest are 0. Binary crossentropy is defined as 
<a href="https://www.codecogs.com/eqnedit.php?latex=L=-[ylog(\hat{p}))&plus;&space;(1-y)log(1-\hat{p})]" target="_blank"><img src="https://latex.codecogs.com/svg.latex?L=-[ylog(\hat{p}))&plus;&space;(1-y)log(1-\hat{p})]" title="L=-[ylog(\hat{p}))+ (1-y)log(1-\hat{p})]" /></a> . The Loss function used was the average of IoU loss and the binary crossentropy loss. The mean of IoU from each sample of the batch was used as the metric function to track the performance of the model. 


#### 2.3.2 Learning Rate Annealing
Learning rate is an important hyperparameter. Basically if the learning rate is too high, the parameter vector will change dramatically, and hard to settle down to get deeper. If the learning rate is too low, it usually takes a long time to train. Usually we need learning rate annealing during neural work training. Here we use the cosine annealing: 

<a href="https://www.codecogs.com/eqnedit.php?latex=lr*\frac{cos(\pi&space;x/n)&plus;1}{2}" target="_blank"><img src="https://latex.codecogs.com/svg.latex?lr*\frac{cos(\pi&space;x/n)&plus;1}{2}" title="lr*\frac{cos(\pi x/n)+1}{2}" /></a>,
<br /> where *lr* is the initial learning rete, *x* is the iteration number, and *n* is the total number of epochs. 

### 2.4 Model Prediction
As montioned in Section 1.2, the output the the model is a numpy arrary which shows the probability of each pixel of the resized image of being in the bounding box. The output required by the challenge is a string which contains the location information of the box and confidence of the box. So first the numpy arrary was resized corresponding to the original image size (1024, 1024, 1). The cut off threshold probability was set to be 0.5. The rectangle region with probability higher than 0.5 were identified and labelled as the predicted boxes. The location of the lower left corner, and hight, width of the rectangle were extracted. The confidience is simply the mean of the probability of all pixels in the bounding rectangle. These confidence, location information of the bounding rectangle were submitted for evaluation. 


